{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installation\n",
    "!git clone https://github.com/CompVis/latent-diffusion.git\n",
    "!git clone https://github.com/CompVis/taming-transformers\n",
    "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install packaging==21.3 torchmetrics==0.7.3\n",
    "\n",
    "!pip install -e ./latent-diffusion\n",
    "!pip install omegaconf>=2.0.0 pytorch-lightning==1.4.2 torch-fidelity einops\n",
    "#!pip install packaging==21.3 torchmetrics==0.7.3\n",
    "\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "sys.path.append('./taming-transformers')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Download model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd latent-diffusion/models/ldm/celeba256\n",
    "!wget https://ommer-lab.com/files/latent-diffusion/celeba.zip\n",
    "#!cd models/ldm/celeba256\n",
    "!unzip -o celeba.zip\n",
    "\n",
    "%cd ../../first_stage_models/vq-f4\n",
    "!wget https://ommer-lab.com/files/latent-diffusion/vq-f4.zip\n",
    "#!cd models/first_stage_models/vq-f4\n",
    "!unzip -o vq-f4.zip\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, datetime, glob, importlib, csv\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from packaging import version\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import random_split, DataLoader, Dataset, Subset\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback, LearningRateMonitor\n",
    "from pytorch_lightning.utilities.distributed import rank_zero_only\n",
    "from pytorch_lightning.utilities import rank_zero_info\n",
    "\n",
    "from ldm.data.base import Txt2ImgIterableBaseDataset\n",
    "from ldm.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nondefault_trainer_args(opt):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args([])\n",
    "    return sorted(k for k in vars(args) if getattr(opt, k) != getattr(args, k))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataset(Dataset):\n",
    "    \"\"\"Wraps an arbitrary object with __len__ and __getitem__ into a pytorch dataset\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModuleFromConfig(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, train=None, validation=None, test=None, predict=None,\n",
    "                 wrap=False, num_workers=None, shuffle_test_loader=False, use_worker_init_fn=False,\n",
    "                 shuffle_val_dataloader=False):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_configs = dict()\n",
    "        self.num_workers = num_workers if num_workers is not None else batch_size * 2\n",
    "        self.use_worker_init_fn = use_worker_init_fn\n",
    "        if train is not None:\n",
    "            self.dataset_configs[\"train\"] = train\n",
    "            self.train_dataloader = self._train_dataloader\n",
    "        if validation is not None:\n",
    "            self.dataset_configs[\"validation\"] = validation\n",
    "            self.val_dataloader = partial(self._val_dataloader, shuffle=shuffle_val_dataloader)\n",
    "        if test is not None:\n",
    "            self.dataset_configs[\"test\"] = test\n",
    "            self.test_dataloader = partial(self._test_dataloader, shuffle=shuffle_test_loader)\n",
    "        if predict is not None:\n",
    "            self.dataset_configs[\"predict\"] = predict\n",
    "            self.predict_dataloader = self._predict_dataloader\n",
    "        self.wrap = wrap\n",
    "\n",
    "    def prepare_data(self):\n",
    "        for data_cfg in self.dataset_configs.values():\n",
    "            instantiate_from_config(data_cfg)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.datasets = dict(\n",
    "            (k, instantiate_from_config(self.dataset_configs[k]))\n",
    "            for k in self.dataset_configs)\n",
    "        if self.wrap:\n",
    "            for k in self.datasets:\n",
    "                self.datasets[k] = WrappedDataset(self.datasets[k])\n",
    "\n",
    "    def _train_dataloader(self):\n",
    "        is_iterable_dataset = isinstance(self.datasets['train'], Txt2ImgIterableBaseDataset)\n",
    "        if is_iterable_dataset or self.use_worker_init_fn:\n",
    "            init_fn = worker_init_fn\n",
    "        else:\n",
    "            init_fn = None\n",
    "        return DataLoader(self.datasets[\"train\"], batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, shuffle=False if is_iterable_dataset else True,\n",
    "                          worker_init_fn=init_fn)\n",
    "\n",
    "    def _val_dataloader(self, shuffle=False):\n",
    "        if isinstance(self.datasets['validation'], Txt2ImgIterableBaseDataset) or self.use_worker_init_fn:\n",
    "            init_fn = worker_init_fn\n",
    "        else:\n",
    "            init_fn = None\n",
    "        return DataLoader(self.datasets[\"validation\"],\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          worker_init_fn=init_fn,\n",
    "                          shuffle=shuffle)\n",
    "\n",
    "    def _test_dataloader(self, shuffle=False):\n",
    "        is_iterable_dataset = isinstance(self.datasets['train'], Txt2ImgIterableBaseDataset)\n",
    "        if is_iterable_dataset or self.use_worker_init_fn:\n",
    "            init_fn = worker_init_fn\n",
    "        else:\n",
    "            init_fn = None\n",
    "\n",
    "        # do not shuffle dataloader for iterable dataset\n",
    "        shuffle = shuffle and (not is_iterable_dataset)\n",
    "\n",
    "        return DataLoader(self.datasets[\"test\"], batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, worker_init_fn=init_fn, shuffle=shuffle)\n",
    "\n",
    "    def _predict_dataloader(self, shuffle=False):\n",
    "        if isinstance(self.datasets['predict'], Txt2ImgIterableBaseDataset) or self.use_worker_init_fn:\n",
    "            init_fn = worker_init_fn\n",
    "        else:\n",
    "            init_fn = None\n",
    "        return DataLoader(self.datasets[\"predict\"], batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, worker_init_fn=init_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetupCallback(Callback):\n",
    "    def __init__(self, resume, now, logdir, ckptdir, cfgdir, config, lightning_config):\n",
    "        super().__init__()\n",
    "        self.resume = resume\n",
    "        self.now = now\n",
    "        self.logdir = logdir\n",
    "        self.ckptdir = ckptdir\n",
    "        self.cfgdir = cfgdir\n",
    "        self.config = config\n",
    "        self.lightning_config = lightning_config\n",
    "\n",
    "    def on_keyboard_interrupt(self, trainer, pl_module):\n",
    "        if trainer.global_rank == 0:\n",
    "            print(\"Summoning checkpoint.\")\n",
    "            ckpt_path = os.path.join(self.ckptdir, \"last.ckpt\")\n",
    "            trainer.save_checkpoint(ckpt_path)\n",
    "\n",
    "    def on_pretrain_routine_start(self, trainer, pl_module):\n",
    "        if trainer.global_rank == 0:\n",
    "            # Create logdirs and save configs\n",
    "            os.makedirs(self.logdir, exist_ok=True)\n",
    "            os.makedirs(self.ckptdir, exist_ok=True)\n",
    "            os.makedirs(self.cfgdir, exist_ok=True)\n",
    "\n",
    "            if \"callbacks\" in self.lightning_config:\n",
    "                if 'metrics_over_trainsteps_checkpoint' in self.lightning_config['callbacks']:\n",
    "                    os.makedirs(os.path.join(self.ckptdir, 'trainstep_checkpoints'), exist_ok=True)\n",
    "            print(\"Project config\")\n",
    "            print(OmegaConf.to_yaml(self.config))\n",
    "            OmegaConf.save(self.config,\n",
    "                           os.path.join(self.cfgdir, \"{}-project.yaml\".format(self.now)))\n",
    "\n",
    "            print(\"Lightning config\")\n",
    "            print(OmegaConf.to_yaml(self.lightning_config))\n",
    "            OmegaConf.save(OmegaConf.create({\"lightning\": self.lightning_config}),\n",
    "                           os.path.join(self.cfgdir, \"{}-lightning.yaml\".format(self.now)))\n",
    "\n",
    "        else:\n",
    "            # ModelCheckpoint callback created log directory --- remove it\n",
    "            if not self.resume and os.path.exists(self.logdir):\n",
    "                dst, name = os.path.split(self.logdir)\n",
    "                dst = os.path.join(dst, \"child_runs\", name)\n",
    "                os.makedirs(os.path.split(dst)[0], exist_ok=True)\n",
    "                try:\n",
    "                    os.rename(self.logdir, dst)\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "\n",
    "\n",
    "class ImageLogger(Callback):\n",
    "    def __init__(self, batch_frequency, max_images, clamp=True, increase_log_steps=True,\n",
    "                 rescale=True, disabled=False, log_on_batch_idx=False, log_first_step=False,\n",
    "                 log_images_kwargs=None):\n",
    "        super().__init__()\n",
    "        self.rescale = rescale\n",
    "        self.batch_freq = batch_frequency\n",
    "        self.max_images = max_images\n",
    "        self.logger_log_images = {\n",
    "            pl.loggers.TestTubeLogger: self._testtube,\n",
    "        }\n",
    "        self.log_steps = [2 ** n for n in range(int(np.log2(self.batch_freq)) + 1)]\n",
    "        if not increase_log_steps:\n",
    "            self.log_steps = [self.batch_freq]\n",
    "        self.clamp = clamp\n",
    "        self.disabled = disabled\n",
    "        self.log_on_batch_idx = log_on_batch_idx\n",
    "        self.log_images_kwargs = log_images_kwargs if log_images_kwargs else {}\n",
    "        self.log_first_step = log_first_step\n",
    "\n",
    "    @rank_zero_only\n",
    "    def _testtube(self, pl_module, images, batch_idx, split):\n",
    "        for k in images:\n",
    "            grid = torchvision.utils.make_grid(images[k])\n",
    "            grid = (grid + 1.0) / 2.0  # -1,1 -> 0,1; c,h,w\n",
    "\n",
    "            tag = f\"{split}/{k}\"\n",
    "            pl_module.logger.experiment.add_image(\n",
    "                tag, grid,\n",
    "                global_step=pl_module.global_step)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_local(self, save_dir, split, images,\n",
    "                  global_step, current_epoch, batch_idx):\n",
    "        root = os.path.join(save_dir, \"images\", split)\n",
    "        for k in images:\n",
    "            grid = torchvision.utils.make_grid(images[k], nrow=4)\n",
    "            if self.rescale:\n",
    "                grid = (grid + 1.0) / 2.0  # -1,1 -> 0,1; c,h,w\n",
    "            grid = grid.transpose(0, 1).transpose(1, 2).squeeze(-1)\n",
    "            grid = grid.numpy()\n",
    "            grid = (grid * 255).astype(np.uint8)\n",
    "            filename = \"{}_gs-{:06}_e-{:06}_b-{:06}.png\".format(\n",
    "                k,\n",
    "                global_step,\n",
    "                current_epoch,\n",
    "                batch_idx)\n",
    "            path = os.path.join(root, filename)\n",
    "            os.makedirs(os.path.split(path)[0], exist_ok=True)\n",
    "            Image.fromarray(grid).save(path)\n",
    "\n",
    "    def log_img(self, pl_module, batch, batch_idx, split=\"train\"):\n",
    "        check_idx = batch_idx if self.log_on_batch_idx else pl_module.global_step\n",
    "        if (self.check_frequency(check_idx) and  # batch_idx % self.batch_freq == 0\n",
    "                hasattr(pl_module, \"log_images\") and\n",
    "                callable(pl_module.log_images) and\n",
    "                self.max_images > 0):\n",
    "            logger = type(pl_module.logger)\n",
    "\n",
    "            is_train = pl_module.training\n",
    "            if is_train:\n",
    "                pl_module.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                images = pl_module.log_images(batch, split=split, **self.log_images_kwargs)\n",
    "\n",
    "            for k in images:\n",
    "                N = min(images[k].shape[0], self.max_images)\n",
    "                images[k] = images[k][:N]\n",
    "                if isinstance(images[k], torch.Tensor):\n",
    "                    images[k] = images[k].detach().cpu()\n",
    "                    if self.clamp:\n",
    "                        images[k] = torch.clamp(images[k], -1., 1.)\n",
    "\n",
    "            self.log_local(pl_module.logger.save_dir, split, images,\n",
    "                           pl_module.global_step, pl_module.current_epoch, batch_idx)\n",
    "\n",
    "            logger_log_images = self.logger_log_images.get(logger, lambda *args, **kwargs: None)\n",
    "            logger_log_images(pl_module, images, pl_module.global_step, split)\n",
    "\n",
    "            if is_train:\n",
    "                pl_module.train()\n",
    "\n",
    "    def check_frequency(self, check_idx):\n",
    "        if ((check_idx % self.batch_freq) == 0 or (check_idx in self.log_steps)) and (\n",
    "                check_idx > 0 or self.log_first_step):\n",
    "            try:\n",
    "                self.log_steps.pop(0)\n",
    "            except IndexError as e:\n",
    "                print(e)\n",
    "                pass\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        if not self.disabled and (pl_module.global_step > 0 or self.log_first_step):\n",
    "            self.log_img(pl_module, batch, batch_idx, split=\"train\")\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        if not self.disabled and pl_module.global_step > 0:\n",
    "            self.log_img(pl_module, batch, batch_idx, split=\"val\")\n",
    "        if hasattr(pl_module, 'calibrate_grad_norm'):\n",
    "            if (pl_module.calibrate_grad_norm and batch_idx % 25 == 0) and batch_idx > 0:\n",
    "                self.log_gradients(trainer, pl_module, batch_idx=batch_idx)\n",
    "\n",
    "\n",
    "class CUDACallback(Callback):\n",
    "    # see https://github.com/SeanNaren/minGPT/blob/master/mingpt/callback.py\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        # Reset the memory use counter\n",
    "        torch.cuda.reset_peak_memory_stats(trainer.root_gpu)\n",
    "        torch.cuda.synchronize(trainer.root_gpu)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module, outputs):\n",
    "        torch.cuda.synchronize(trainer.root_gpu)\n",
    "        max_memory = torch.cuda.max_memory_allocated(trainer.root_gpu) / 2 ** 20\n",
    "        epoch_time = time.time() - self.start_time\n",
    "\n",
    "        try:\n",
    "            max_memory = trainer.training_type_plugin.reduce(max_memory)\n",
    "            epoch_time = trainer.training_type_plugin.reduce(epoch_time)\n",
    "\n",
    "            rank_zero_info(f\"Average Epoch time: {epoch_time:.2f} seconds\")\n",
    "            rank_zero_info(f\"Average Peak memory {max_memory:.2f}MiB\")\n",
    "        except AttributeError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.Namespace()\n",
    "\n",
    "# Setting default parser variables\n",
    "parser.name = \"\"\n",
    "parser.resume = \"\"\n",
    "parser.base = \"models/ldm/cin256/config.yaml\"\n",
    "parser.train = True\n",
    "parser.no_test = False\n",
    "parser.debug = False\n",
    "parser.seed = 23\n",
    "parser.postfix = \"\"\n",
    "parser.logdir= \"logs\"\n",
    "parser.scale_lr = True\n",
    "\n",
    "#PyTorch-Lightning argument\n",
    "parser.accelerator = \"gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "\n",
    "# add cwd for convenience and to make classes in this file available when\n",
    "# running as `python main.py`\n",
    "# (in particular `main.DataModuleFromConfig`)\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "#parser = get_parser()\n",
    "parser = Trainer.add_argparse_args(parser)\n",
    "\n",
    "opt, unknown = parser.parse_known_args()\n",
    "if opt.name and opt.resume:\n",
    "    raise ValueError(\n",
    "        \"-n/--name and -r/--resume cannot be specified both.\"\n",
    "        \"If you want to resume training in a new log folder, \"\n",
    "        \"use -n/--name in combination with --resume_from_checkpoint\"\n",
    "    )\n",
    "if opt.resume:\n",
    "    if not os.path.exists(opt.resume):\n",
    "        raise ValueError(\"Cannot find {}\".format(opt.resume))\n",
    "    if os.path.isfile(opt.resume):\n",
    "        paths = opt.resume.split(\"/\")\n",
    "        # idx = len(paths)-paths[::-1].index(\"logs\")+1\n",
    "        # logdir = \"/\".join(paths[:idx])\n",
    "        logdir = \"/\".join(paths[:-2])\n",
    "        ckpt = opt.resume\n",
    "    else:\n",
    "        assert os.path.isdir(opt.resume), opt.resume\n",
    "        logdir = opt.resume.rstrip(\"/\")\n",
    "        ckpt = os.path.join(logdir, \"checkpoints\", \"last.ckpt\")\n",
    "\n",
    "    opt.resume_from_checkpoint = ckpt\n",
    "    base_configs = sorted(glob.glob(os.path.join(logdir, \"configs/*.yaml\")))\n",
    "    opt.base = base_configs + opt.base\n",
    "    _tmp = logdir.split(\"/\")\n",
    "    nowname = _tmp[-1]\n",
    "else:\n",
    "    if opt.name:\n",
    "        name = \"_\" + opt.name\n",
    "    elif opt.base:\n",
    "        cfg_fname = os.path.split(opt.base[0])[-1]\n",
    "        cfg_name = os.path.splitext(cfg_fname)[0]\n",
    "        name = \"_\" + cfg_name\n",
    "    else:\n",
    "        name = \"\"\n",
    "    nowname = now + name + opt.postfix\n",
    "    logdir = os.path.join(opt.logdir, nowname)\n",
    "\n",
    "ckptdir = os.path.join(logdir, \"checkpoints\")\n",
    "cfgdir = os.path.join(logdir, \"configs\")\n",
    "seed_everything(opt.seed)\n",
    "\n",
    "try:\n",
    "    # init and save configs\n",
    "    configs = [OmegaConf.load(cfg) for cfg in opt.base]\n",
    "    cli = OmegaConf.from_dotlist(unknown)\n",
    "    config = OmegaConf.merge(*configs, cli)\n",
    "    lightning_config = config.pop(\"lightning\", OmegaConf.create())\n",
    "    # merge trainer cli with config\n",
    "    trainer_config = lightning_config.get(\"trainer\", OmegaConf.create())\n",
    "    # default to ddp\n",
    "    trainer_config[\"accelerator\"] = \"ddp\"\n",
    "    for k in nondefault_trainer_args(opt):\n",
    "        trainer_config[k] = getattr(opt, k)\n",
    "    if not \"gpus\" in trainer_config:\n",
    "        del trainer_config[\"accelerator\"]\n",
    "        cpu = True\n",
    "    else:\n",
    "        gpuinfo = trainer_config[\"gpus\"]\n",
    "        print(f\"Running on GPUs {gpuinfo}\")\n",
    "        cpu = False\n",
    "    trainer_opt = argparse.Namespace(**trainer_config)\n",
    "    lightning_config.trainer = trainer_config\n",
    "\n",
    "    # model\n",
    "    model = instantiate_from_config(config.model)\n",
    "\n",
    "    # trainer and callbacks\n",
    "    trainer_kwargs = dict()\n",
    "\n",
    "    # default logger configs\n",
    "    default_logger_cfgs = {\n",
    "        \"wandb\": {\n",
    "            \"target\": \"pytorch_lightning.loggers.WandbLogger\",\n",
    "            \"params\": {\n",
    "                \"name\": nowname,\n",
    "                \"save_dir\": logdir,\n",
    "                \"offline\": opt.debug,\n",
    "                \"id\": nowname,\n",
    "            }\n",
    "        },\n",
    "        \"testtube\": {\n",
    "            \"target\": \"pytorch_lightning.loggers.TestTubeLogger\",\n",
    "            \"params\": {\n",
    "                \"name\": \"testtube\",\n",
    "                \"save_dir\": logdir,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    default_logger_cfg = default_logger_cfgs[\"testtube\"]\n",
    "    if \"logger\" in lightning_config:\n",
    "        logger_cfg = lightning_config.logger\n",
    "    else:\n",
    "        logger_cfg = OmegaConf.create()\n",
    "    logger_cfg = OmegaConf.merge(default_logger_cfg, logger_cfg)\n",
    "    trainer_kwargs[\"logger\"] = instantiate_from_config(logger_cfg)\n",
    "\n",
    "    # modelcheckpoint - use TrainResult/EvalResult(checkpoint_on=metric) to\n",
    "    # specify which metric is used to determine best models\n",
    "    default_modelckpt_cfg = {\n",
    "        \"target\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\n",
    "        \"params\": {\n",
    "            \"dirpath\": ckptdir,\n",
    "            \"filename\": \"{epoch:06}\",\n",
    "            \"verbose\": True,\n",
    "            \"save_last\": True,\n",
    "        }\n",
    "    }\n",
    "    if hasattr(model, \"monitor\"):\n",
    "        print(f\"Monitoring {model.monitor} as checkpoint metric.\")\n",
    "        default_modelckpt_cfg[\"params\"][\"monitor\"] = model.monitor\n",
    "        default_modelckpt_cfg[\"params\"][\"save_top_k\"] = 3\n",
    "\n",
    "    if \"modelcheckpoint\" in lightning_config:\n",
    "        modelckpt_cfg = lightning_config.modelcheckpoint\n",
    "    else:\n",
    "        modelckpt_cfg =  OmegaConf.create()\n",
    "    modelckpt_cfg = OmegaConf.merge(default_modelckpt_cfg, modelckpt_cfg)\n",
    "    print(f\"Merged modelckpt-cfg: \\n{modelckpt_cfg}\")\n",
    "    if version.parse(pl.__version__) < version.parse('1.4.0'):\n",
    "        trainer_kwargs[\"checkpoint_callback\"] = instantiate_from_config(modelckpt_cfg)\n",
    "\n",
    "    # add callback which sets up log directory\n",
    "    default_callbacks_cfg = {\n",
    "        \"setup_callback\": {\n",
    "            \"target\": \"main.SetupCallback\",\n",
    "            \"params\": {\n",
    "                \"resume\": opt.resume,\n",
    "                \"now\": now,\n",
    "                \"logdir\": logdir,\n",
    "                \"ckptdir\": ckptdir,\n",
    "                \"cfgdir\": cfgdir,\n",
    "                \"config\": config,\n",
    "                \"lightning_config\": lightning_config,\n",
    "            }\n",
    "        },\n",
    "        \"image_logger\": {\n",
    "            \"target\": \"main.ImageLogger\",\n",
    "            \"params\": {\n",
    "                \"batch_frequency\": 750,\n",
    "                \"max_images\": 4,\n",
    "                \"clamp\": True\n",
    "            }\n",
    "        },\n",
    "        \"learning_rate_logger\": {\n",
    "            \"target\": \"main.LearningRateMonitor\",\n",
    "            \"params\": {\n",
    "                \"logging_interval\": \"step\",\n",
    "                # \"log_momentum\": True\n",
    "            }\n",
    "        },\n",
    "        \"cuda_callback\": {\n",
    "            \"target\": \"main.CUDACallback\"\n",
    "        },\n",
    "    }\n",
    "    if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
    "        default_callbacks_cfg.update({'checkpoint_callback': modelckpt_cfg})\n",
    "\n",
    "    if \"callbacks\" in lightning_config:\n",
    "        callbacks_cfg = lightning_config.callbacks\n",
    "    else:\n",
    "        callbacks_cfg = OmegaConf.create()\n",
    "\n",
    "    if 'metrics_over_trainsteps_checkpoint' in callbacks_cfg:\n",
    "        print(\n",
    "            'Caution: Saving checkpoints every n train steps without deleting. This might require some free space.')\n",
    "        default_metrics_over_trainsteps_ckpt_dict = {\n",
    "            'metrics_over_trainsteps_checkpoint':\n",
    "                {\"target\": 'pytorch_lightning.callbacks.ModelCheckpoint',\n",
    "                 'params': {\n",
    "                     \"dirpath\": os.path.join(ckptdir, 'trainstep_checkpoints'),\n",
    "                     \"filename\": \"{epoch:06}-{step:09}\",\n",
    "                     \"verbose\": True,\n",
    "                     'save_top_k': -1,\n",
    "                     'every_n_train_steps': 10000,\n",
    "                     'save_weights_only': True\n",
    "                 }\n",
    "                 }\n",
    "        }\n",
    "        default_callbacks_cfg.update(default_metrics_over_trainsteps_ckpt_dict)\n",
    "\n",
    "    callbacks_cfg = OmegaConf.merge(default_callbacks_cfg, callbacks_cfg)\n",
    "    if 'ignore_keys_callback' in callbacks_cfg and hasattr(trainer_opt, 'resume_from_checkpoint'):\n",
    "        callbacks_cfg.ignore_keys_callback.params['ckpt_path'] = trainer_opt.resume_from_checkpoint\n",
    "    elif 'ignore_keys_callback' in callbacks_cfg:\n",
    "        del callbacks_cfg['ignore_keys_callback']\n",
    "\n",
    "    trainer_kwargs[\"callbacks\"] = [instantiate_from_config(callbacks_cfg[k]) for k in callbacks_cfg]\n",
    "\n",
    "    trainer = Trainer.from_argparse_args(trainer_opt, **trainer_kwargs)\n",
    "    trainer.logdir = logdir  ###\n",
    "\n",
    "    # data\n",
    "    data = instantiate_from_config(config.data)\n",
    "    # NOTE according to https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n",
    "    # calling these ourselves should not be necessary but it is.\n",
    "    # lightning still takes care of proper multiprocessing though\n",
    "    data.prepare_data()\n",
    "    data.setup()\n",
    "    print(\"#### Data #####\")\n",
    "    for k in data.datasets:\n",
    "        print(f\"{k}, {data.datasets[k].__class__.__name__}, {len(data.datasets[k])}\")\n",
    "\n",
    "    # configure learning rate\n",
    "    bs, base_lr = config.data.params.batch_size, config.model.base_learning_rate\n",
    "    if not cpu:\n",
    "        ngpu = len(lightning_config.trainer.gpus.strip(\",\").split(','))\n",
    "    else:\n",
    "        ngpu = 1\n",
    "    if 'accumulate_grad_batches' in lightning_config.trainer:\n",
    "        accumulate_grad_batches = lightning_config.trainer.accumulate_grad_batches\n",
    "    else:\n",
    "        accumulate_grad_batches = 1\n",
    "    print(f\"accumulate_grad_batches = {accumulate_grad_batches}\")\n",
    "    lightning_config.trainer.accumulate_grad_batches = accumulate_grad_batches\n",
    "    if opt.scale_lr:\n",
    "        model.learning_rate = accumulate_grad_batches * ngpu * bs * base_lr\n",
    "        print(\n",
    "            \"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\".format(\n",
    "                model.learning_rate, accumulate_grad_batches, ngpu, bs, base_lr))\n",
    "    else:\n",
    "        model.learning_rate = base_lr\n",
    "        print(\"++++ NOT USING LR SCALING ++++\")\n",
    "        print(f\"Setting learning rate to {model.learning_rate:.2e}\")\n",
    "\n",
    "\n",
    "    # allow checkpointing via USR1\n",
    "    def melk(*args, **kwargs):\n",
    "        # run all checkpoint hooks\n",
    "        if trainer.global_rank == 0:\n",
    "            print(\"Summoning checkpoint.\")\n",
    "            ckpt_path = os.path.join(ckptdir, \"last.ckpt\")\n",
    "            trainer.save_checkpoint(ckpt_path)\n",
    "\n",
    "\n",
    "    def divein(*args, **kwargs):\n",
    "        if trainer.global_rank == 0:\n",
    "            import pudb;\n",
    "            pudb.set_trace()\n",
    "\n",
    "\n",
    "    import signal\n",
    "\n",
    "    signal.signal(signal.SIGUSR1, melk)\n",
    "    signal.signal(signal.SIGUSR2, divein)\n",
    "\n",
    "    # run\n",
    "    if opt.train:\n",
    "        try:\n",
    "            trainer.fit(model, data)\n",
    "        except Exception:\n",
    "            melk()\n",
    "            raise\n",
    "    if not opt.no_test and not trainer.interrupted:\n",
    "        trainer.test(model, data)\n",
    "except Exception:\n",
    "    if opt.debug and trainer.global_rank == 0:\n",
    "        try:\n",
    "            import pudb as debugger\n",
    "        except ImportError:\n",
    "            import pdb as debugger\n",
    "        debugger.post_mortem()\n",
    "    raise\n",
    "finally:\n",
    "    # move newly created debug project to debug_runs\n",
    "    if opt.debug and not opt.resume and trainer.global_rank == 0:\n",
    "        dst, name = os.path.split(logdir)\n",
    "        dst = os.path.join(dst, \"debug_runs\", name)\n",
    "        os.makedirs(os.path.split(dst)[0], exist_ok=True)\n",
    "        os.rename(logdir, dst)\n",
    "    if trainer.global_rank == 0:\n",
    "        print(trainer.profiler.summary())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
